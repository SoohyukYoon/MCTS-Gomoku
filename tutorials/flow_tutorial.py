# -*- coding: utf-8 -*-
"""flow_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kUm5AijGCeKTpHa08ABNXgxOaqdYYOfI

# Tutorial: How to Train Neural Networks

Training a neural network for **optical flow** using the **MPI‑Sintel** dataset.

Focus:
- Data visualization
- Custom dataset classes
- Training loops
- Debugging model training
- Finetuning

## Optical Flow

Definition: Optical flow is the apparent motion of brightness patterns inthe image. [Source](https://slazebni.cs.illinois.edu/fall24/lec08_optical_flow.pdf)

<img src="./figures/flow.png" width="800">

<img src="./figures/flow_example.png" width="800">
"""

import os
import math
import random
import time
import pathlib
from typing import Dict, Tuple, Optional, List

import glob
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split

SEED = 42
random.seed(SEED); np.random.seed(SEED)
torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print (device)

"""## MPI-Sintel Dataset

Set `DATA_ROOT` to where you've placed a (possibly small) Sintel subset.

Expected layout (per split + sequence):

```
/path/to/sintel/
  training/
    clean/SEQUENCE/frame_0001.png, frame_0002.png, ...
    final/SEQUENCE/frame_0001.png, frame_0002.png, ...
    flow/SEQUENCE/frame_0001.flo, frame_0002.flo, ...
```
"""

!wget http://files.is.tue.mpg.de/sintel/MPI-Sintel-complete.zip
!mkdir -p datasets/sintel
!unzip MPI-Sintel-complete.zip -d datasets/sintel

DATA_ROOT = "./datasets/sintel"
PASS = "clean"
print("DATA_ROOT:", DATA_ROOT, "PASS:", PASS)

"""## Utilities: read `.flo`, visualize flow"""

def read_flo(path):
    """Read optical flow from Middlebury .flo file."""
    with open(path, 'rb') as f:
        magic = np.fromfile(f, np.float32, count=1)
        if magic.size != 1 or magic[0] != 202021.25:
            raise ValueError(f'Invalid .flo file: {path}')
        w = int(np.fromfile(f, np.int32, count=1)[0])
        h = int(np.fromfile(f, np.int32, count=1)[0])
        data = np.fromfile(f, np.float32, count=2*w*h)
        if data.size != 2*w*h:
            raise ValueError(f'Truncated .flo payload: {path}')
    return data.reshape(h, w, 2)

# HSV to RGB conversion for flow visualization
def flow_to_rgb_hsv(flow, clip=10.0):
    """Convert flow (H,W,2) to an RGB image for visualization."""
    import numpy as np, colorsys
    u = np.clip(flow[...,0], -clip, clip) / clip
    v = np.clip(flow[...,1], -clip, clip) / clip
    mag = np.sqrt(u**2 + v**2)
    ang = np.arctan2(v, u)  # [-pi, pi]
    hsv = np.zeros((*flow.shape[:2], 3), dtype=np.float32)
    hsv[...,0] = (ang + np.pi) / (2*np.pi)
    hsv[...,1] = 1.0
    hsv[...,2] = np.clip(mag, 0, 1)
    rgb = np.zeros_like(hsv)
    for i in range(hsv.shape[0]):
        for j in range(hsv.shape[1]):
            rgb[i,j] = colorsys.hsv_to_rgb(hsv[i,j,0], hsv[i,j,1], hsv[i,j,2])
    return rgb

# RGB conversion using Middlebury color wheel
def flow_to_rgb(flow: np.ndarray, maxmotion: float = -1.0) -> np.ndarray:
    """
    Middlebury/C++-exact flow color coding.
    - flow[...,0]=u, flow[...,1]=v (px/frame)
    - maxmotion>0: use as normalization; else use max over known pixels
    - Unknown = |u|>1e9 or |v|>1e9 or NaN/Inf -> black
    Returns: (H,W,3) uint8 RGB
    """
    u = flow[..., 0].astype(np.float32, copy=False)
    v = flow[..., 1].astype(np.float32, copy=False)

    # 5) Unknown handling: match C++ (unknown -> black)
    unknown = (np.abs(u) > 1e9) | (np.abs(v) > 1e9) | ~np.isfinite(u) | ~np.isfinite(v)

    # 4) Normalization (MotionToColor behavior)
    rad_all = np.sqrt(u*u + v*v)
    if maxmotion is not None and maxmotion > 0:
        maxrad = float(maxmotion)
    else:
        valid = rad_all[~unknown]
        maxrad = float(valid.max()) if valid.size else 0.0
    if maxrad == 0.0:
        maxrad = 1.0

    fx = u / maxrad
    fy = v / maxrad

    # 1) Wheel with **integer truncation** exactly like C++
    def _make_colorwheel():
        RY, YG, GC, CB, BM, MR = 15, 6, 4, 11, 13, 6
        ncols = RY + YG + GC + CB + BM + MR  # 55
        cw = np.zeros((ncols, 3), dtype=np.float32)
        k = 0
        # RY: (255, 0->255, 0)
        for i in range(RY): cw[k+i] = (255, int(255*i/RY), 0)
        k += RY
        # YG: (255->0, 255, 0)
        for i in range(YG): cw[k+i] = (int(255-255*i/YG), 255, 0)
        k += YG
        # GC: (0, 255, 0->255)
        for i in range(GC): cw[k+i] = (0, 255, int(255*i/GC))
        k += GC
        # CB: (0, 255->0, 255)
        for i in range(CB): cw[k+i] = (0, int(255-255*i/CB), 255)
        k += CB
        # BM: (0->255, 0, 255)
        for i in range(BM): cw[k+i] = (int(255*i/BM), 0, 255)
        k += BM
        # MR: (255, 0, 255->0)
        for i in range(MR): cw[k+i] = (255, 0, int(255-255*i/MR))
        return cw / 255.0  # store as 0..1 floats like the C++ normalized after /255
    if not hasattr(flow_to_rgb, "_wheel"):
        flow_to_rgb._wheel = _make_colorwheel()
    colorwheel = flow_to_rgb._wheel
    ncols = colorwheel.shape[0]

    # 3) Angle convention: **atan2(-fy, -fx) / pi** on normalized flow
    a = np.arctan2(-fy, -fx) / np.pi  # [-1,1]
    fk = (a + 1.0) / 2.0 * (ncols - 1)  # [0, ncols-1]
    k0 = np.floor(fk).astype(np.int32)
    k1 = (k0 + 1) % ncols
    f  = (fk - k0).astype(np.float32)

    rad = np.sqrt(fx*fx + fy*fy)

    # 2) Channel order: assign to out[..., 2 - c] (like pix[2-b] in C++)
    out = np.zeros(u.shape + (3,), dtype=np.float32)
    for c in range(3):
        col0 = colorwheel[k0, c]
        col1 = colorwheel[k1, c]
        col  = (1.0 - f) * col0 + f * col1
        # Middlebury saturation rule
        col  = np.where(rad <= 1.0, 1.0 - rad * (1.0 - col), col * 0.75)
        out[..., c] = col

    # 5) Unknown -> black
    out[unknown] = 0.0

    # 6) Final packing uses **truncation** (not rounding), like (int)(255*col) in C++
    return np.clip(out * 255.0, 0, 255).astype(np.uint8)

"""## Visualizing Flow Data

Converting flow values [u, v] to RGB using colorwheel.

<img src="./figures/flow_colorwheel_middlebury.png" width="220">
"""

seq_name = "alley_1"
img_dir = os.path.join(DATA_ROOT, "training", PASS, seq_name)
flow_dir = os.path.join(DATA_ROOT, "training", "flow", seq_name)
img_paths = sorted(glob.glob(os.path.join(img_dir, "*.png")))

t = 0
img1 = Image.open(img_paths[t])
img2 = Image.open(img_paths[t+1])
name = os.path.basename(img_paths[t])
flow = read_flo(os.path.join(flow_dir, name.replace(".png", ".flo")))

flow_viz_dir = os.path.join(DATA_ROOT, "training", "flow_viz", seq_name)
flow_viz = Image.open(os.path.join(flow_viz_dir, name))

plt.figure(figsize=(16,8))
plt.subplot(1,4,1)
plt.imshow(img1)
plt.title("Image 1")
plt.axis('off')
plt.subplot(1,4,2)
plt.imshow(img2)
plt.title("Image 2")
plt.axis('off')
plt.subplot(1,4,3)
# plt.imshow(flow_to_rgb_hsv(flow))
plt.imshow(flow_to_rgb(flow))
plt.title("Flow")
plt.axis('off')
plt.subplot(1,4,4)
plt.imshow(flow_viz)
plt.title("Flow GT")
plt.axis('off')
plt.show()

"""## Dataset class (Sintel pairs)

Pairs `(frame_t, frame_{t+1}) -> flow_t`. Includes random flips/crops.
"""

class SintelFlow(Dataset):
    def __init__(self, root, split='training', img_pass='clean',
                 crop_size=256, augment=True, debug=False, seed=123):
        self.root = root
        self.split = split
        self.img_pass = img_pass
        self.crop = int(crop_size)
        self.augment = bool(augment)
        self.debug = bool(debug)

        base = pathlib.Path(root) / split
        img_dir = base / img_pass
        flow_dir = base / 'flow'

        if not img_dir.exists(): raise RuntimeError(f"Images path not found: {img_dir}")
        if not flow_dir.exists(): raise RuntimeError(f"Flow path not found: {flow_dir}")

        self.samples = []
        for seq in sorted(os.listdir(img_dir)):
            seq_img_dir = img_dir / seq
            if not seq_img_dir.is_dir():
                continue
            frames = sorted(glob.glob(str(seq_img_dir / "*.png")))
            for k in range(len(frames)-1):
                f1 = frames[k]
                f2 = frames[k+1]
                flow_name = os.path.basename(frames[k]).replace(".png", ".flo")
                flo = str(flow_dir / seq / flow_name)
                if os.path.exists(flo):
                    self.samples.append((f1, f2, flo))

        if len(self.samples) == 0:
            raise RuntimeError("No samples found. Check DATA_ROOT layout.")

        # Deterministic behavior for debug / single-sample overfit
        self.rng = random.Random(seed)

    def __len__(self):
        return 1 if self.debug else len(self.samples)

    def _to_tensor(self, img: Image.Image):
        arr = np.array(img, dtype=np.float32) / 255.0  # H,W,3
        return torch.from_numpy(arr.transpose(2,0,1))  # 3,H,W

    def _resize_flow_torch(self, flow_hw2: np.ndarray, new_h: int, new_w: int) -> np.ndarray:
        """
        Properly resample flow field spatially with bilinear interpolation and scale vectors.
        flow_hw2: (H,W,2) np.float32
        """
        H, W, _ = flow_hw2.shape
        flow = torch.from_numpy(flow_hw2.transpose(2,0,1)).unsqueeze(0)  # [1,2,H,W]
        flow = flow.float()
        # scale components first (vector magnitude), then resample grid
        sx, sy = new_w / W, new_h / H
        flow[:,0] *= sx
        flow[:,1] *= sy
        flow = F.interpolate(flow, size=(new_h, new_w), mode='bilinear', align_corners=False)
        return flow.squeeze(0).permute(1,2,0).cpu().numpy()

    def __getitem__(self, idx):
        f1, f2, flo = self.samples[idx]
        im1 = Image.open(f1).convert('RGB')
        im2 = Image.open(f2).convert('RGB')
        flow = read_flo(flo).astype(np.float32)  # H,W,2

        W, H = im1.size
        ch = cw = self.crop

        # --- Horizontal flip (only if augment=True and not debug) ---
        if self.augment and not self.debug and self.rng.random() < 0.5:
            im1 = im1.transpose(Image.FLIP_LEFT_RIGHT)
            im2 = im2.transpose(Image.FLIP_LEFT_RIGHT)
            flow = flow[:, ::-1, :]
            flow[..., 0] *= -1.0

        # --- Crop / Resize ---
        if H >= ch and W >= cw:
            if self.debug:
                # deterministic center crop for overfit sanity
                y0 = (H - ch) // 2
                x0 = (W - cw) // 2
            else:
                y0 = self.rng.randint(0, H - ch)
                x0 = self.rng.randint(0, W - cw)
            im1 = im1.crop((x0, y0, x0+cw, y0+ch))
            im2 = im2.crop((x0, y0, x0+cw, y0+ch))
            flow = flow[y0:y0+ch, x0:x0+cw, :]
        else:
            # resize images and PROPERLY resample flow field
            im1 = im1.resize((cw, ch), Image.BILINEAR)
            im2 = im2.resize((cw, ch), Image.BILINEAR)
            flow = self._resize_flow_torch(flow, ch, cw)

        # ensure contiguous arrays (slicing sometimes yields non-contiguous)
        flow = np.ascontiguousarray(flow)

        im1 = self._to_tensor(im1)
        im2 = self._to_tensor(im2)
        flow = torch.from_numpy(flow.transpose(2,0,1))  # 2,H,W

        return {'im1': im1, 'im2': im2, 'flow': flow}

CROP = 256
debug = False
full = SintelFlow(DATA_ROOT, split='training', img_pass=PASS, crop_size=CROP, augment=True, debug=debug)
n_total = len(full)
n_val = max(1, int(0.1 * n_total))
n_train = n_total - n_val
print (f"Total samples: {n_total}, Train: {n_train}, Val: {n_val}")

# Visualize some samples
def show_sample(sample):
    im1 = sample['im1'].numpy().transpose(1,2,0)
    im2 = sample['im2'].numpy().transpose(1,2,0)
    flow = sample['flow'].numpy().transpose(1,2,0)
    flow_rgb = flow_to_rgb(flow)

    plt.figure(figsize=(10,4))
    plt.subplot(1,3,1); plt.imshow(im1); plt.title('Image 1'); plt.axis('off')
    plt.subplot(1,3,2); plt.imshow(im2); plt.title('Image 2'); plt.axis('off')
    plt.subplot(1,3,3); plt.imshow(flow_rgb); plt.title('Flow'); plt.axis('off')
    plt.show()

viz_samples = 2
for i in range(viz_samples):
    sample = full[random.randint(0, len(full)-1)]
    show_sample(sample)

"""## FlowNet model

FlowNet: Learning Optical Flow with Convolutional Networks. ICCV 2015 [Paper](https://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf)

Convolution model for predicting optical flow. [Source Code](https://github.com/ClementPinard/FlowNetPytorch/)

<img src="./figures/flownet.png" width="800">
"""

from torch.nn.init import kaiming_normal_, constant_

def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):
    if batchNorm:
        return nn.Sequential(
            nn.Conv2d(
                in_planes,
                out_planes,
                kernel_size=kernel_size,
                stride=stride,
                padding=(kernel_size - 1) // 2,
                # bias=False,
            ),
            nn.BatchNorm2d(out_planes),
            nn.LeakyReLU(0.1, inplace=True),
        )
    else:
        return nn.Sequential(
            nn.Conv2d(
                in_planes,
                out_planes,
                kernel_size=kernel_size,
                stride=stride,
                padding=(kernel_size - 1) // 2,
                # bias=True,
            ),
            nn.LeakyReLU(0.1, inplace=True),
        )


def predict_flow(in_planes):
    return nn.Conv2d(in_planes, 2, kernel_size=3, stride=1, padding=1, bias=True)


def deconv(in_planes, out_planes):
    return nn.Sequential(
        nn.ConvTranspose2d(
            in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=True
        ),
        nn.LeakyReLU(0.1, inplace=True),
    )


def crop_like(input, target):
    if input.size()[2:] == target.size()[2:]:
        return input
    else:
        return input[:, :, : target.size(2), : target.size(3)]


class FlowNetS(nn.Module):
    expansion = 1

    def __init__(self, batchNorm=True):
        super(FlowNetS, self).__init__()

        self.batchNorm = batchNorm
        self.conv1 = conv(self.batchNorm, 6, 64, kernel_size=7, stride=2)
        self.conv2 = conv(self.batchNorm, 64, 128, kernel_size=5, stride=2)
        self.conv3 = conv(self.batchNorm, 128, 256, kernel_size=5, stride=2)
        self.conv3_1 = conv(self.batchNorm, 256, 256)
        self.conv4 = conv(self.batchNorm, 256, 512, stride=2)
        self.conv4_1 = conv(self.batchNorm, 512, 512)
        self.conv5 = conv(self.batchNorm, 512, 512, stride=2)
        self.conv5_1 = conv(self.batchNorm, 512, 512)
        self.conv6 = conv(self.batchNorm, 512, 1024, stride=2)
        self.conv6_1 = conv(self.batchNorm, 1024, 1024)

        self.deconv5 = deconv(1024, 512)
        self.deconv4 = deconv(1026, 256)
        self.deconv3 = deconv(770, 128)
        self.deconv2 = deconv(386, 64)
        self.deconv1 = deconv(64, 64)

        self.predict_flow6 = predict_flow(1024)
        self.predict_flow5 = predict_flow(1026)
        self.predict_flow4 = predict_flow(770)
        self.predict_flow3 = predict_flow(386)
        self.predict_flow2 = predict_flow(194)
        self.predict_flow1 = predict_flow(64)

        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow2_to_1 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                kaiming_normal_(m.weight, 0.1)
                if m.bias is not None:
                    constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                constant_(m.weight, 1)
                constant_(m.bias, 0)

    def forward(self, x):
        out_conv2 = self.conv2(self.conv1(x))
        out_conv3 = self.conv3_1(self.conv3(out_conv2))
        out_conv4 = self.conv4_1(self.conv4(out_conv3))
        out_conv5 = self.conv5_1(self.conv5(out_conv4))
        out_conv6 = self.conv6_1(self.conv6(out_conv5))

        flow6 = self.predict_flow6(out_conv6)
        flow6_up = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)
        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)

        concat5 = torch.cat((out_conv5, out_deconv5, flow6_up), 1)
        flow5 = self.predict_flow5(concat5)
        flow5_up = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)
        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)

        concat4 = torch.cat((out_conv4, out_deconv4, flow5_up), 1)
        flow4 = self.predict_flow4(concat4)
        flow4_up = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)
        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)

        concat3 = torch.cat((out_conv3, out_deconv3, flow4_up), 1)
        flow3 = self.predict_flow3(concat3)
        flow3_up = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)
        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)

        concat2 = torch.cat((out_conv2, out_deconv2, flow3_up), 1)
        flow2 = self.predict_flow2(concat2)

        out_deconv1 = self.deconv1(out_deconv2)
        flow1 = self.upsampled_flow2_to_1(self.predict_flow1(out_deconv1))

        if self.training:
            return flow1, flow2, flow3, flow4, flow5, flow6
        else:
            return flow1

    def weight_parameters(self):
        return [param for name, param in self.named_parameters() if "weight" in name]

    def bias_parameters(self):
        return [param for name, param in self.named_parameters() if "bias" in name]

"""## Losses and metrics"""

def epe_loss(pred, gt):
    return torch.norm(pred - gt, dim=1).mean()

def smoothness_loss(flow):
    dx = flow[:, :, :, 1:] - flow[:, :, :, :-1]
    dy = flow[:, :, 1:, :] - flow[:, :, :-1, :]
    return (dx.abs().mean() + dy.abs().mean())

@torch.no_grad()
def epe_metric(pred, gt):
    return torch.norm(pred - gt, dim=1).mean().item()

"""## Train / Validate"""

class FlowTrainer:
    def __init__(
        self,
        model: nn.Module,
        device: torch.device,
        train_loader,
        val_loader,
        optimizer: torch.optim.Optimizer,
        epe_loss_fn,            # callable(pred, gt) -> scalar
        epe_metric_fn,          # callable(pred, gt) -> scalar
        smoothness_loss_fn=None,# callable(pred) -> scalar  (can be None)
        lambda_smooth: float = 0.0,
        ckpt_path: str = "best.pt",
        grad_clip: Optional[float] = None,
        log_interval: int = 250,
        debug: bool = False,
    ):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.epe_loss_fn = epe_loss_fn
        self.epe_metric_fn = epe_metric_fn
        self.smoothness_loss_fn = smoothness_loss_fn
        self.lambda_smooth = float(lambda_smooth)
        self.ckpt_path = ckpt_path
        self.grad_clip = grad_clip
        self.log_interval = log_interval
        self.debug = debug

        self.history: Dict[str, List[float]] = {
            'all_loss': [], 'train_loss': [], 'train_epe': [],
            'val_loss': [], 'val_epe': []
        }
        self.best_val_epe = float('inf')

    def _forward_concat(self, im1, im2):
        x = torch.cat([im1, im2], dim=1)  # [B,6,H,W]
        out = self.model(x)
        if isinstance(out, (tuple, list)):
            out = out[0]  # full-res flow1
        return out

    def set_loaders(self, train_loader, val_loader):
        self.train_loader = train_loader
        self.val_loader = val_loader

    def train_one_epoch(self, return_all_loss: bool = False):
        self.model.train()
        loss_total, epe_total = 0.0, 0.0
        all_loss = []

        for idx, batch in enumerate(self.train_loader):
            im1 = batch['im1'].to(self.device)
            im2 = batch['im2'].to(self.device)
            gt  = batch['flow'].to(self.device)

            pred = self._forward_concat(im1, im2)
            loss = self.epe_loss_fn(pred, gt)
            if self.smoothness_loss_fn is not None and self.lambda_smooth > 0:
                loss = loss + self.lambda_smooth * self.smoothness_loss_fn(pred)

            self.optimizer.zero_grad()
            loss.backward()
            if self.grad_clip is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)
            self.optimizer.step()

            val = float(loss.item())
            all_loss.append(val)
            loss_total += val
            epe_total  += float(self.epe_metric_fn(pred, gt))

            if self.log_interval and (idx + 1) % self.log_interval == 0:
                print(f"  batch {idx+1:03d}/{len(self.train_loader)} | "
                      f"loss {loss_total/(idx+1):.3f} | epe {epe_total/(idx+1):.3f}")

        avg_loss = loss_total / max(1, len(self.train_loader))
        avg_epe  = epe_total  / max(1, len(self.train_loader))
        if return_all_loss:
            return (all_loss, avg_loss), avg_epe
        return avg_loss, avg_epe

    @torch.no_grad()
    def evaluate(self, loader=None):
        loader = loader or self.val_loader
        self.model.eval()
        loss_total, epe_total = 0.0, 0.0
        for batch in loader:
            im1 = batch['im1'].to(self.device, non_blocking=True)
            im2 = batch['im2'].to(self.device, non_blocking=True)
            gt  = batch['flow'].to(self.device, non_blocking=True)
            pred = self._forward_concat(im1, im2)
            loss_total += float(self.epe_loss_fn(pred, gt).item())
            epe_total  += float(self.epe_metric_fn(pred, gt))
        avg_loss = loss_total / max(1, len(loader))
        avg_epe  = epe_total  / max(1, len(loader))
        return avg_loss, avg_epe

    def fit(self, epochs: int, tag: str = "", return_all_loss: bool = True):
        print(f">>> Training {f'({tag}) ' if tag else ''}for {epochs} epoch(s)")
        for epoch in range(1, epochs + 1):
            t0 = time.time()
            tr_loss, tr_epe = self.train_one_epoch(return_all_loss=return_all_loss)
            if isinstance(tr_loss, tuple):
                all_loss, tr_loss = tr_loss
                self.history['all_loss'] += all_loss
            va_loss, va_epe = self.evaluate(self.val_loader)

            self.history['train_loss'].append(tr_loss)
            self.history['train_epe'].append(tr_epe)
            self.history['val_loss'].append(va_loss)
            self.history['val_epe'].append(va_epe)

            if va_epe < self.best_val_epe:
                self.best_val_epe = va_epe
                torch.save({'state': self.model.state_dict()}, self.ckpt_path)

            dt = time.time() - t0
            if self.debug:
                if epoch % 50 == 0:
                    print(f"{'['+tag+']' if tag else ''} epoch {epoch:02d} | {dt:5.1f}s | "
                        f"train loss {tr_loss:.3f} | train EPE {tr_epe:.3f} | "
                        f"val loss {va_loss:.3f} | val EPE {va_epe:.3f}")
            else:
                print(f"{'['+tag+']' if tag else ''} epoch {epoch:02d} | {dt:5.1f}s | "
                    f"train loss {tr_loss:.3f} | train EPE {tr_epe:.3f} | "
                    f"val loss {va_loss:.3f} | val EPE {va_epe:.3f}")

        print("Best val EPE:", self.best_val_epe)
        return self.history

model = FlowNetS(batchNorm=False).to(device)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Model total trainable parameters: {total_params/1e6:.2f}M")

num_workers = 8
BATCH_SIZE = 32
LEARNING_RATE = 3e-4
EPOCHS = 10
LAMBDA_SMOOTH = 0.1
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

debug = False
print ('debug mode:', debug)
if debug:
    BATCH_SIZE = 1
    num_workers = 0
    full = SintelFlow(DATA_ROOT, split='training', img_pass=PASS, crop_size=CROP, augment=False, debug=debug)
    train_ds = full
    val_ds = full
    n_train = len(train_ds)
    n_val = len(val_ds)
    EPOCHS = 1000
    LAMBDA_SMOOTH = 0.0
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
else:
    full = SintelFlow(DATA_ROOT, split='training', img_pass=PASS, crop_size=CROP, augment=True)
    train_ds, val_ds = random_split(full, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))
    val_ds.augment = False

print (f"Train dataset size: {len(train_ds)}")
print (f"Val dataset size: {len(val_ds)}")
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)

model = FlowNetS(batchNorm=False).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

trainer = FlowTrainer(
    model=model,
    device=device,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    epe_loss_fn=epe_loss,
    epe_metric_fn=epe_metric,
    smoothness_loss_fn=smoothness_loss,
    lambda_smooth=LAMBDA_SMOOTH,
    ckpt_path="best_sintel_flownet.pt",
    grad_clip=None,
    log_interval=250,
    debug=debug
)

history = trainer.fit(epochs=EPOCHS, tag="sintel_flownet", return_all_loss=True)

def plot_train_loss(history):
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(history.get('all_loss', []), label='train loss')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Loss')
    ax.set_title('Training Curve')
    ax.legend()
    fig.tight_layout()
    return fig, ax

fig, ax = plot_train_loss(history)
plt.show()

def plot_metrics(history):
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(history.get('train_epe', []), label='train EPE')
    ax.plot(history.get('val_epe', []),   label='val EPE')
    ax.set_xlabel('epoch')
    ax.set_ylabel('EPE (lower is better)')
    ax.set_title('Metrics')
    ax.legend()
    fig.tight_layout()
    return fig, ax

fig, ax = plot_metrics(history)
plt.show()

"""## Qualitative visualization"""

@torch.no_grad()
def visualize_batch(n=1):
    model.eval()
    batch = next(iter(val_loader))
    im1 = batch['im1'][:n].to(device)
    im2 = batch['im2'][:n].to(device)
    gt  = batch['flow'][:n].to(device)
    im = torch.cat([im1, im2], dim=1)
    pred = model(im)

    def inv_img(t):
        return np.clip(t.detach().cpu().permute(1,2,0).numpy(), 0, 1)

    for i in range(n):
        f_pred = pred[i].detach().cpu().permute(1,2,0).numpy()
        f_gt   = gt[i].detach().cpu().permute(1,2,0).numpy()
        rgb_pred = flow_to_rgb(f_pred)
        rgb_gt   = flow_to_rgb(f_gt)
        epe = np.linalg.norm(f_pred - f_gt, axis=-1)

        fig, axs = plt.subplots(1,5, figsize=(16,3))
        axs[0].imshow(inv_img(im1[i])); axs[0].set_title('im1'); axs[0].axis('off')
        axs[1].imshow(inv_img(im2[i])); axs[1].set_title('im2'); axs[1].axis('off')
        axs[2].imshow(rgb_gt);          axs[2].set_title('GT flow'); axs[2].axis('off')
        axs[3].imshow(rgb_pred);        axs[3].set_title('Pred flow'); axs[3].axis('off')
        im = axs[4].imshow(epe, cmap='magma'); axs[4].set_title('EPE heat'); axs[4].axis('off')
        plt.colorbar(im, ax=axs[4], fraction=0.046, pad=0.04)
        plt.tight_layout()
        plt.show()

visualize_batch()

"""# Pretrain on FlyingChairs → Finetune on MPI-Sintel

This section adds a two-stage schedule for optical flow:

1. **Pretrain** on **FlyingChairs** (synthetic).  
2. **Finetune** on **MPI-Sintel** (clean).
"""

# The data is too big, not necessary to download
# We have already trained on it and provided the weights
!wget https://lmb.informatik.uni-freiburg.de/data/FlyingChairs/FlyingChairs.zip
!unzip FlyingChairs.zip -d datasets/

"""## Datasets & loaders"""

class FlyingChairs(Dataset):
    def __init__(self, root, crop_size=256, augment=True, debug=False, seed=123):
        root = pathlib.Path(root)
        data_dir = root / "data"
        self.root = data_dir if data_dir.exists() else root

        self.crop = int(crop_size)
        self.augment = bool(augment)
        self.debug = bool(debug)
        self.rng = random.Random(seed)

        # 2) Collect samples robustly
        patt = str(self.root / "*_img1.ppm")
        img1_list = sorted(glob.glob(patt), key=lambda p: pathlib.Path(p).stem)
        self.samples = []
        for f1 in img1_list:
            stem = pathlib.Path(f1).name.replace("_img1.ppm", "")
            f2  = self.root / f"{stem}_img2.ppm"
            flo = self.root / f"{stem}_flow.flo"
            if f2.exists() and flo.exists():
                self.samples.append((str(f1), str(f2), str(flo)))

        if not self.samples:
            raise RuntimeError(f"No FlyingChairs samples found under {self.root}")

    def __len__(self):
        return 1 if self.debug else len(self.samples)

    def _to_tensor(self, img: Image.Image):
        arr = np.array(img, dtype=np.float32) / 255.0  # H,W,3
        return torch.from_numpy(arr.transpose(2,0,1))  # 3,H,W

    def _resize_flow_torch(self, flow_hw2: np.ndarray, new_h: int, new_w: int) -> np.ndarray:
        H, W, _ = flow_hw2.shape
        flow = torch.from_numpy(flow_hw2.transpose(2,0,1)).unsqueeze(0).float()  # [1,2,H,W]
        sx, sy = new_w / W, new_h / H
        flow[:,0] *= sx
        flow[:,1] *= sy
        flow = F.interpolate(flow, size=(new_h, new_w), mode='bilinear', align_corners=False)
        return flow.squeeze(0).permute(1,2,0).contiguous().cpu().numpy()

    def __getitem__(self, idx):
        # 3) Deterministic index for debug/overfit
        if self.debug:
            idx = 0

        f1, f2, flo = self.samples[idx]
        im1 = Image.open(f1).convert('RGB')
        im2 = Image.open(f2).convert('RGB')
        flow = read_flo(flo).astype(np.float32)  # H,W,2

        W, H = im1.size
        ch = cw = self.crop

        # 4) Optional horiz flip (disabled in debug)
        if self.augment and not self.debug and self.rng.random() < 0.5:
            im1 = im1.transpose(Image.FLIP_LEFT_RIGHT)
            im2 = im2.transpose(Image.FLIP_LEFT_RIGHT)
            flow = flow[:, ::-1, :].copy()   # ensure contiguous
            flow[..., 0] *= -1.0

        # 5) Crop / Resize
        if H >= ch and W >= cw:
            if self.debug:
                y0 = (H - ch) // 2
                x0 = (W - cw) // 2
            else:
                y0 = self.rng.randint(0, H - ch)
                x0 = self.rng.randint(0, W - cw)
            im1 = im1.crop((x0, y0, x0+cw, y0+ch))
            im2 = im2.crop((x0, y0, x0+cw, y0+ch))
            flow = flow[y0:y0+ch, x0:x0+cw, :].copy()
        else:
            im1 = im1.resize((cw, ch), Image.BILINEAR)
            im2 = im2.resize((cw, ch), Image.BILINEAR)
            flow = self._resize_flow_torch(flow, ch, cw)

        im1 = self._to_tensor(im1)                 # [3,H,W]
        im2 = self._to_tensor(im2)
        flow = torch.from_numpy(flow.transpose(2,0,1))  # [2,H,W]

        return {'im1': im1, 'im2': im2, 'flow': flow}

# ---- Configure roots (EDIT THESE) ----
CHAIRS_ROOT = "./datasets/FlyingChairs_release"
SINTEL_ROOT = "./datasets/sintel"

CROP = 256
BATCH_CHAIRS = 32
BATCH_SINTEL = 32

# FlyingChairs loaders
chairs_full = FlyingChairs(CHAIRS_ROOT, crop_size=CROP, augment=True)
n_val = max(1, int(0.1*len(chairs_full)))
n_train = len(chairs_full) - n_val
chairs_train, chairs_val = random_split(chairs_full, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))

chairs_train_loader = DataLoader(chairs_train, batch_size=BATCH_CHAIRS, shuffle=True,  num_workers=8)
chairs_val_loader   = DataLoader(chairs_val,   batch_size=BATCH_CHAIRS, shuffle=False, num_workers=8)

print('Chairs train/val sizes:', len(chairs_train), len(chairs_val))

# Sintel loaders
SINTEL_PASS = "clean"
sintel_train_ds = SintelFlow(SINTEL_ROOT, split='training', img_pass=SINTEL_PASS, crop_size=CROP, augment=True)
n_val_s = max(1, int(0.1*len(sintel_train_ds)))
n_tr_s  = len(sintel_train_ds) - n_val_s
sintel_train, sintel_val = random_split(sintel_train_ds, [n_tr_s, n_val_s], generator=torch.Generator().manual_seed(SEED))

sintel_train_loader = DataLoader(sintel_train, batch_size=BATCH_SINTEL, shuffle=True,  num_workers=8)
sintel_val_loader   = DataLoader(sintel_val,   batch_size=BATCH_SINTEL, shuffle=False, num_workers=8)

print('Sintel train/val sizes:', len(sintel_train), len(sintel_val))

"""## Pretrain on FlyingChairs (reuse earlier Trainer class)"""

models_chairs = FlowNetS(batchNorm=False).to(device)
total_params = sum(p.numel() for p in models_chairs.parameters() if p.requires_grad)
print(f"Model total trainable parameters: {total_params/1e6:.2f}M")

EPOCHS_PRE = 20
LAMBDA_SMOOTH = 0.1
optimizer = torch.optim.AdamW(models_chairs.parameters(), lr=1e-4)

trainer = FlowTrainer(
    model=models_chairs,
    device=device,
    train_loader=chairs_train_loader,
    val_loader=chairs_val_loader,
    optimizer=optimizer,
    epe_loss_fn=epe_loss,
    epe_metric_fn=epe_metric,
    smoothness_loss_fn=smoothness_loss,
    lambda_smooth=LAMBDA_SMOOTH,
    ckpt_path="chairs_pretrained.pt",
    grad_clip=None,
    log_interval=250,
    debug=debug
)

history_chairs = trainer.fit(epochs=EPOCHS_PRE, tag="chairs_flownet", return_all_loss=True)

fig, ax = plot_train_loss(history_chairs)
plt.show()

fig, ax = plot_metrics(history_chairs)
plt.show()

"""## Finetune on MPI-Sintel"""

model_sintel_ft = FlowNetS(batchNorm=False).to(device)
total_params = sum(p.numel() for p in model_sintel_ft.parameters() if p.requires_grad)
print(f"Model total trainable parameters: {total_params/1e6:.2f}M")

LEARNING_RATE = 0.0003
EPOCHS_FT = 10
LAMBDA_SMOOTH = 0.1
# Load pretrained weights
state = torch.load('chairs_pretrained.pt', map_location=device)
model_sintel_ft.load_state_dict(state['state'])

optimizer = torch.optim.AdamW(model_sintel_ft.parameters(), lr=LEARNING_RATE)

trainer = FlowTrainer(
    model=model_sintel_ft,
    device=device,
    train_loader=sintel_train_loader,
    val_loader=sintel_val_loader,
    optimizer=optimizer,
    epe_loss_fn=epe_loss,
    epe_metric_fn=epe_metric,
    smoothness_loss_fn=smoothness_loss,
    lambda_smooth=LAMBDA_SMOOTH,
    ckpt_path="best_sintel_flownet_ft.pt",
    grad_clip=None,
    log_interval=250,
    debug=debug
)

history_sintel_ft = trainer.fit(epochs=EPOCHS_FT, tag="sintel_flownet_ft", return_all_loss=True)

fig, ax = plot_train_loss(history_sintel_ft)
plt.show()

fig, ax = plot_metrics(history_sintel_ft)
plt.show()

"""## FlowNet Model with shared encoder"""

!export CUDA_HOME=/usr/local/cuda-12.4/ # change to your cuda path
!pip install spatial-correlation-sampler>=0.2.1

from spatial_correlation_sampler import spatial_correlation_sample
def correlate(input1, input2):
    out_corr = spatial_correlation_sample(
        input1,
        input2,
        kernel_size=1,
        patch_size=21,
        stride=1,
        padding=0,
        dilation_patch=2,
    )
    # collate dimensions 1 and 2 in order to be treated as a
    # regular 4D tensor
    b, ph, pw, h, w = out_corr.size()
    out_corr = out_corr.view(b, ph * pw, h, w) / input1.size(1)
    return F.leaky_relu_(out_corr, 0.1)

class FlowNetC(nn.Module):
    expansion = 1

    def __init__(self, batchNorm=True):
        super(FlowNetC, self).__init__()

        self.batchNorm = batchNorm
        self.conv1 = conv(self.batchNorm, 3, 64, kernel_size=7, stride=2)
        self.conv2 = conv(self.batchNorm, 64, 128, kernel_size=5, stride=2)
        self.conv3 = conv(self.batchNorm, 128, 256, kernel_size=5, stride=2)
        self.conv_redir = conv(self.batchNorm, 256, 32, kernel_size=1, stride=1)

        self.conv3_1 = conv(self.batchNorm, 473, 256)
        self.conv4 = conv(self.batchNorm, 256, 512, stride=2)
        self.conv4_1 = conv(self.batchNorm, 512, 512)
        self.conv5 = conv(self.batchNorm, 512, 512, stride=2)
        self.conv5_1 = conv(self.batchNorm, 512, 512)
        self.conv6 = conv(self.batchNorm, 512, 1024, stride=2)
        self.conv6_1 = conv(self.batchNorm, 1024, 1024)

        self.deconv5 = deconv(1024, 512)
        self.deconv4 = deconv(1026, 256)
        self.deconv3 = deconv(770, 128)
        self.deconv2 = deconv(386, 64)

        self.predict_flow6 = predict_flow(1024)
        self.predict_flow5 = predict_flow(1026)
        self.predict_flow4 = predict_flow(770)
        self.predict_flow3 = predict_flow(386)
        self.predict_flow2 = predict_flow(194)

        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)
        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                kaiming_normal_(m.weight, 0.1)
                if m.bias is not None:
                    constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                constant_(m.weight, 1)
                constant_(m.bias, 0)

    def forward(self, x):
        x1 = x[:, :3]
        x2 = x[:, 3:]

        out_conv1a = self.conv1(x1)
        out_conv2a = self.conv2(out_conv1a)
        out_conv3a = self.conv3(out_conv2a)

        out_conv1b = self.conv1(x2)
        out_conv2b = self.conv2(out_conv1b)
        out_conv3b = self.conv3(out_conv2b)

        out_conv_redir = self.conv_redir(out_conv3a)
        out_correlation = correlate(out_conv3a, out_conv3b)

        in_conv3_1 = torch.cat([out_conv_redir, out_correlation], dim=1)

        out_conv3 = self.conv3_1(in_conv3_1)
        out_conv4 = self.conv4_1(self.conv4(out_conv3))
        out_conv5 = self.conv5_1(self.conv5(out_conv4))
        out_conv6 = self.conv6_1(self.conv6(out_conv5))

        flow6 = self.predict_flow6(out_conv6)
        flow6_up = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)
        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)

        concat5 = torch.cat((out_conv5, out_deconv5, flow6_up), 1)
        flow5 = self.predict_flow5(concat5)
        flow5_up = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)
        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)

        concat4 = torch.cat((out_conv4, out_deconv4, flow5_up), 1)
        flow4 = self.predict_flow4(concat4)
        flow4_up = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)
        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)

        concat3 = torch.cat((out_conv3, out_deconv3, flow4_up), 1)
        flow3 = self.predict_flow3(concat3)
        flow3_up = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2a)
        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2a)

        concat2 = torch.cat((out_conv2a, out_deconv2, flow3_up), 1)
        flow2 = self.predict_flow2(concat2)

        if self.training:
            return flow2, flow3, flow4, flow5, flow6
        else:
            return flow2

    def weight_parameters(self):
        return [param for name, param in self.named_parameters() if "weight" in name]

    def bias_parameters(self):
        return [param for name, param in self.named_parameters() if "bias" in name]

model_shared = FlowNetC(batchNorm=False).to(device)
optimizer = torch.optim.AdamW(model_shared.parameters(), lr=3e-4)

trainer = FlowTrainer(
    model=model_shared,
    device=device,
    train_loader=chairs_train_loader,
    val_loader=chairs_val_loader,
    optimizer=optimizer,
    epe_loss_fn=epe_loss,
    epe_metric_fn=epe_metric,
    smoothness_loss_fn=smoothness_loss,
    lambda_smooth=LAMBDA_SMOOTH,
    ckpt_path="chairs_pretrained.pt",
    grad_clip=None,
    log_interval=250,
)

history_sintel_shared = trainer.fit(epochs=50, tag="sintel_flownet_shared", return_all_loss=True)